{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hairy-prerequisite",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tf_pose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-ac2afb09ed8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib_plot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlib_plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib_commons\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlib_commons\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib_openpose\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSkeletonDetector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib_tracker\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTracker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib_tracker\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTracker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Realtime-Action-Recognition\\src/..\\utils\\lib_openpose.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# openpose packages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mROOT\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"src/githubs/tf-pose-estimation\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtf_pose\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetworks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_graph_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_wh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtf_pose\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfPoseEstimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtf_pose\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf_pose'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "'''\n",
    "Test action recognition on\n",
    "(1) a video, (2) a folder of images, (3) or web camera.\n",
    "\n",
    "Input:\n",
    "    model: model/trained_classifier.pickle\n",
    "\n",
    "Output:\n",
    "    result video:    output/${video_name}/video.avi\n",
    "    result skeleton: output/${video_name}/skeleton_res/XXXXX.txt\n",
    "    visualization by cv2.imshow() in img_displayer\n",
    "'''\n",
    "\n",
    "'''\n",
    "Example of usage:\n",
    "\n",
    "(1) Test on video file:\n",
    "python src/s5_test.py \\\n",
    "    --model_path model/trained_classifier.pickle \\\n",
    "    --data_type video \\\n",
    "    --data_path data_test/exercise.avi \\\n",
    "    --output_folder output\n",
    "    \n",
    "(2) Test on a folder of images:\n",
    "python src/s5_test.py \\\n",
    "    --model_path model/trained_classifier.pickle \\\n",
    "    --data_type folder \\\n",
    "    --data_path data_test/apple/ \\\n",
    "    --output_folder output\n",
    "\n",
    "(3) Test on web camera:\n",
    "python src/s5_test.py \\\n",
    "    --model_path model/trained_classifier.pickle \\\n",
    "    --data_type webcam \\\n",
    "    --data_path 0 \\\n",
    "    --output_folder output\n",
    "    \n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import argparse\n",
    "if True:  # Include project path\n",
    "    import sys\n",
    "    import os\n",
    "    ROOT = os.path.dirname(os.path.abspath(\"__file__\"))+\"/../\"\n",
    "    CURR_PATH = os.path.dirname(os.path.abspath(\"__file__\"))+\"/\"\n",
    "    sys.path.append(ROOT)\n",
    "\n",
    "    import utils.lib_images_io as lib_images_io\n",
    "    import utils.lib_plot as lib_plot\n",
    "    import utils.lib_commons as lib_commons\n",
    "    from utils.lib_openpose import SkeletonDetector\n",
    "    from utils.lib_tracker import Tracker\n",
    "    from utils.lib_tracker import Tracker\n",
    "    from utils.lib_classifier import ClassifierOnlineTest\n",
    "    from utils.lib_classifier import *  # Import all sklearn related libraries\n",
    "\n",
    "\n",
    "def par(path):  # Pre-Append ROOT to the path if it's not absolute\n",
    "    return ROOT + path if (path and path[0] != \"/\") else path\n",
    "\n",
    "\n",
    "# -- Command-line input\n",
    "\n",
    "\n",
    "def get_command_line_arguments():\n",
    "\n",
    "    def parse_args():\n",
    "        parser = argparse.ArgumentParser(\n",
    "            description=\"Test action recognition on \\n\"\n",
    "            \"(1) a video, (2) a folder of images, (3) or web camera.\")\n",
    "        parser.add_argument(\"-m\", \"--model_path\", required=False,\n",
    "                            default='model/trained_classifier.pickle')\n",
    "        parser.add_argument(\"-t\", \"--data_type\", required=False, default='webcam',\n",
    "                            choices=[\"video\", \"folder\", \"webcam\"])\n",
    "        parser.add_argument(\"-p\", \"--data_path\", required=False, default=\"\",\n",
    "                            help=\"path to a video file, or images folder, or webcam. \\n\"\n",
    "                            \"For video and folder, the path should be \"\n",
    "                            \"absolute or relative to this project's root. \"\n",
    "                            \"For webcam, either input an index or device name. \")\n",
    "        parser.add_argument(\"-o\", \"--output_folder\", required=False, default='output/',\n",
    "                            help=\"Which folder to save result to.\")\n",
    "\n",
    "        args = parser.parse_args()\n",
    "        return args\n",
    "    args = parse_args()\n",
    "    if args.data_type != \"webcam\" and args.data_path and args.data_path[0] != \"/\":\n",
    "        # If the path is not absolute, then its relative to the ROOT.\n",
    "        args.data_path = ROOT + args.data_path\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_dst_folder_name(src_data_type, src_data_path):\n",
    "    ''' Compute a output folder name based on data_type and data_path.\n",
    "        The final output of this script looks like this:\n",
    "            DST_FOLDER/folder_name/vidoe.avi\n",
    "            DST_FOLDER/folder_name/skeletons/XXXXX.txt\n",
    "    '''\n",
    "\n",
    "    assert(src_data_type in [\"video\", \"folder\", \"webcam\"])\n",
    "\n",
    "    if src_data_type == \"video\":  # /root/data/video.avi --> video\n",
    "        folder_name = os.path.basename(src_data_path).split(\".\")[-2]\n",
    "\n",
    "    elif src_data_type == \"folder\":  # /root/data/video/ --> video\n",
    "        folder_name = src_data_path.rstrip(\"/\").split(\"/\")[-1]\n",
    "\n",
    "    elif src_data_type == \"webcam\":\n",
    "        # month-day-hour-minute-seconds, e.g.: 02-26-15-51-12\n",
    "        folder_name = lib_commons.get_time_string()\n",
    "\n",
    "    return folder_name\n",
    "\n",
    "\n",
    "args = get_command_line_arguments()\n",
    "\n",
    "SRC_DATA_TYPE = args.data_type\n",
    "SRC_DATA_PATH = args.data_path\n",
    "SRC_MODEL_PATH = args.model_path\n",
    "\n",
    "DST_FOLDER_NAME = get_dst_folder_name(SRC_DATA_TYPE, SRC_DATA_PATH)\n",
    "\n",
    "# -- Settings\n",
    "\n",
    "cfg_all = lib_commons.read_yaml(ROOT + \"config/config.yaml\")\n",
    "cfg = cfg_all[\"s5_test.py\"]\n",
    "\n",
    "CLASSES = np.array(cfg_all[\"classes\"])\n",
    "SKELETON_FILENAME_FORMAT = cfg_all[\"skeleton_filename_format\"]\n",
    "\n",
    "# Action recognition: number of frames used to extract features.\n",
    "WINDOW_SIZE = int(cfg_all[\"features\"][\"window_size\"])\n",
    "\n",
    "# Output folder\n",
    "DST_FOLDER = args.output_folder + \"/\" + DST_FOLDER_NAME + \"/\"\n",
    "DST_SKELETON_FOLDER_NAME = cfg[\"output\"][\"skeleton_folder_name\"]\n",
    "DST_VIDEO_NAME = cfg[\"output\"][\"video_name\"]\n",
    "# framerate of output video.avi\n",
    "DST_VIDEO_FPS = float(cfg[\"output\"][\"video_fps\"])\n",
    "\n",
    "\n",
    "# Video setttings\n",
    "\n",
    "# If data_type is webcam, set the max frame rate.\n",
    "SRC_WEBCAM_MAX_FPS = float(cfg[\"settings\"][\"source\"]\n",
    "                           [\"webcam_max_framerate\"])\n",
    "\n",
    "# If data_type is video, set the sampling interval.\n",
    "# For example, if it's 3, then the video will be read 3 times faster.\n",
    "SRC_VIDEO_SAMPLE_INTERVAL = int(cfg[\"settings\"][\"source\"]\n",
    "                                [\"video_sample_interval\"])\n",
    "\n",
    "# Openpose settings\n",
    "OPENPOSE_MODEL = cfg[\"settings\"][\"openpose\"][\"model\"]\n",
    "OPENPOSE_IMG_SIZE = cfg[\"settings\"][\"openpose\"][\"img_size\"]\n",
    "\n",
    "# Display settings\n",
    "img_disp_desired_rows = int(cfg[\"settings\"][\"display\"][\"desired_rows\"])\n",
    "\n",
    "\n",
    "# -- Function\n",
    "\n",
    "\n",
    "def select_images_loader(src_data_type, src_data_path):\n",
    "    if src_data_type == \"video\":\n",
    "        images_loader = lib_images_io.ReadFromVideo(\n",
    "            src_data_path,\n",
    "            sample_interval=SRC_VIDEO_SAMPLE_INTERVAL)\n",
    "\n",
    "    elif src_data_type == \"folder\":\n",
    "        images_loader = lib_images_io.ReadFromFolder(\n",
    "            folder_path=src_data_path)\n",
    "\n",
    "    elif src_data_type == \"webcam\":\n",
    "        if src_data_path == \"\":\n",
    "            webcam_idx = 0\n",
    "        elif src_data_path.isdigit():\n",
    "            webcam_idx = int(src_data_path)\n",
    "        else:\n",
    "            webcam_idx = src_data_path\n",
    "        images_loader = lib_images_io.ReadFromWebcam(\n",
    "            SRC_WEBCAM_MAX_FPS, webcam_idx)\n",
    "    return images_loader\n",
    "\n",
    "\n",
    "class MultiPersonClassifier(object):\n",
    "    ''' This is a wrapper around ClassifierOnlineTest\n",
    "        for recognizing actions of multiple people.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model_path, classes):\n",
    "\n",
    "        self.dict_id2clf = {}  # human id -> classifier of this person\n",
    "\n",
    "        # Define a function for creating classifier for new people.\n",
    "        self._create_classifier = lambda human_id: ClassifierOnlineTest(\n",
    "            model_path, classes, WINDOW_SIZE, human_id)\n",
    "\n",
    "    def classify(self, dict_id2skeleton):\n",
    "        ''' Classify the action type of each skeleton in dict_id2skeleton '''\n",
    "\n",
    "        # Clear people not in view\n",
    "        old_ids = set(self.dict_id2clf)\n",
    "        cur_ids = set(dict_id2skeleton)\n",
    "        humans_not_in_view = list(old_ids - cur_ids)\n",
    "        for human in humans_not_in_view:\n",
    "            del self.dict_id2clf[human]\n",
    "\n",
    "        # Predict each person's action\n",
    "        id2label = {}\n",
    "        for id, skeleton in dict_id2skeleton.items():\n",
    "\n",
    "            if id not in self.dict_id2clf:  # add this new person\n",
    "                self.dict_id2clf[id] = self._create_classifier(id)\n",
    "\n",
    "            classifier = self.dict_id2clf[id]\n",
    "            id2label[id] = classifier.predict(skeleton)  # predict label\n",
    "            # print(\"\\n\\nPredicting label for human{}\".format(id))\n",
    "            # print(\"  skeleton: {}\".format(skeleton))\n",
    "            # print(\"  label: {}\".format(id2label[id]))\n",
    "\n",
    "        return id2label\n",
    "\n",
    "    def get_classifier(self, id):\n",
    "        ''' Get the classifier based on the person id.\n",
    "        Arguments:\n",
    "            id {int or \"min\"}\n",
    "        '''\n",
    "        if len(self.dict_id2clf) == 0:\n",
    "            return None\n",
    "        if id == 'min':\n",
    "            id = min(self.dict_id2clf.keys())\n",
    "        return self.dict_id2clf[id]\n",
    "\n",
    "\n",
    "def remove_skeletons_with_few_joints(skeletons):\n",
    "    ''' Remove bad skeletons before sending to the tracker '''\n",
    "    good_skeletons = []\n",
    "    for skeleton in skeletons:\n",
    "        px = skeleton[2:2+13*2:2]\n",
    "        py = skeleton[3:2+13*2:2]\n",
    "        num_valid_joints = len([x for x in px if x != 0])\n",
    "        num_leg_joints = len([x for x in px[-6:] if x != 0])\n",
    "        total_size = max(py) - min(py)\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        # IF JOINTS ARE MISSING, TRY CHANGING THESE VALUES:\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        if num_valid_joints >= 5 and total_size >= 0.1 and num_leg_joints >= 0:\n",
    "            # add this skeleton only when all requirements are satisfied\n",
    "            good_skeletons.append(skeleton)\n",
    "    return good_skeletons\n",
    "\n",
    "\n",
    "def draw_result_img(img_disp, ith_img, humans, dict_id2skeleton,\n",
    "                    skeleton_detector, multiperson_classifier):\n",
    "    ''' Draw skeletons, labels, and prediction scores onto image for display '''\n",
    "\n",
    "    # Resize to a proper size for display\n",
    "    r, c = img_disp.shape[0:2]\n",
    "    desired_cols = int(1.0 * c * (img_disp_desired_rows / r))\n",
    "    img_disp = cv2.resize(img_disp,\n",
    "                          dsize=(desired_cols, img_disp_desired_rows))\n",
    "\n",
    "    # Draw all people's skeleton\n",
    "    skeleton_detector.draw(img_disp, humans)\n",
    "\n",
    "    # Draw bounding box and label of each person\n",
    "    if len(dict_id2skeleton):\n",
    "        for id, label in dict_id2label.items():\n",
    "            skeleton = dict_id2skeleton[id]\n",
    "            # scale the y data back to original\n",
    "            skeleton[1::2] = skeleton[1::2] / scale_h\n",
    "            # print(\"Drawing skeleton: \", dict_id2skeleton[id], \"with label:\", label, \".\")\n",
    "            lib_plot.draw_action_result(img_disp, id, skeleton, label)\n",
    "\n",
    "    # Add blank to the left for displaying prediction scores of each class\n",
    "    img_disp = lib_plot.add_white_region_to_left_of_image(img_disp)\n",
    "\n",
    "    cv2.putText(img_disp, \"Frame:\" + str(ith_img),\n",
    "                (20, 20), fontScale=1.5, fontFace=cv2.FONT_HERSHEY_PLAIN,\n",
    "                color=(0, 0, 0), thickness=2)\n",
    "\n",
    "    # Draw predicting score for only 1 person\n",
    "    if len(dict_id2skeleton):\n",
    "        classifier_of_a_person = multiperson_classifier.get_classifier(\n",
    "            id='min')\n",
    "        classifier_of_a_person.draw_scores_onto_image(img_disp)\n",
    "    return img_disp\n",
    "\n",
    "\n",
    "def get_the_skeleton_data_to_save_to_disk(dict_id2skeleton):\n",
    "    '''\n",
    "    In each image, for each skeleton, save the:\n",
    "        human_id, label, and the skeleton positions of length 18*2.\n",
    "    So the total length per row is 2+36=38\n",
    "    '''\n",
    "    skels_to_save = []\n",
    "    for human_id in dict_id2skeleton.keys():\n",
    "        label = dict_id2label[human_id]\n",
    "        skeleton = dict_id2skeleton[human_id]\n",
    "        skels_to_save.append([[human_id, label] + skeleton.tolist()])\n",
    "    return skels_to_save\n",
    "\n",
    "\n",
    "# -- Main\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # -- Detector, tracker, classifier\n",
    "\n",
    "    skeleton_detector = SkeletonDetector(OPENPOSE_MODEL, OPENPOSE_IMG_SIZE)\n",
    "\n",
    "    multiperson_tracker = Tracker()\n",
    "\n",
    "    multiperson_classifier = MultiPersonClassifier(SRC_MODEL_PATH, CLASSES)\n",
    "\n",
    "    # -- Image reader and displayer\n",
    "    images_loader = select_images_loader(SRC_DATA_TYPE, SRC_DATA_PATH)\n",
    "    img_displayer = lib_images_io.ImageDisplayer()\n",
    "\n",
    "    # -- Init output\n",
    "\n",
    "    # output folder\n",
    "    os.makedirs(DST_FOLDER, exist_ok=True)\n",
    "    os.makedirs(DST_FOLDER + DST_SKELETON_FOLDER_NAME, exist_ok=True)\n",
    "\n",
    "    # video writer\n",
    "    video_writer = lib_images_io.VideoWriter(\n",
    "        DST_FOLDER + DST_VIDEO_NAME, DST_VIDEO_FPS)\n",
    "\n",
    "    # -- Read images and process\n",
    "    try:\n",
    "        ith_img = -1\n",
    "        while images_loader.has_image():\n",
    "\n",
    "            # -- Read image\n",
    "            img = images_loader.read_image()\n",
    "            ith_img += 1\n",
    "            img_disp = img.copy()\n",
    "            print(f\"\\nProcessing {ith_img}th image ...\")\n",
    "\n",
    "            # -- Detect skeletons\n",
    "            humans = skeleton_detector.detect(img)\n",
    "            skeletons, scale_h = skeleton_detector.humans_to_skels_list(humans)\n",
    "            skeletons = remove_skeletons_with_few_joints(skeletons)\n",
    "\n",
    "            # -- Track people\n",
    "            dict_id2skeleton = multiperson_tracker.track(\n",
    "                skeletons)  # int id -> np.array() skeleton\n",
    "\n",
    "            # -- Recognize action of each person\n",
    "            if len(dict_id2skeleton):\n",
    "                dict_id2label = multiperson_classifier.classify(\n",
    "                    dict_id2skeleton)\n",
    "\n",
    "            # -- Draw\n",
    "            img_disp = draw_result_img(img_disp, ith_img, humans, dict_id2skeleton,\n",
    "                                       skeleton_detector, multiperson_classifier)\n",
    "\n",
    "            # Print label of a person\n",
    "            if len(dict_id2skeleton):\n",
    "                min_id = min(dict_id2skeleton.keys())\n",
    "                print(\"prediced label is :\", dict_id2label[min_id])\n",
    "\n",
    "            # -- Display image, and write to video.avi\n",
    "            img_displayer.display(img_disp, wait_key_ms=1)\n",
    "            video_writer.write(img_disp)\n",
    "\n",
    "            # -- Get skeleton data and save to file\n",
    "            skels_to_save = get_the_skeleton_data_to_save_to_disk(\n",
    "                dict_id2skeleton)\n",
    "            lib_commons.save_listlist(\n",
    "                DST_FOLDER + DST_SKELETON_FOLDER_NAME +\n",
    "                SKELETON_FILENAME_FORMAT.format(ith_img),\n",
    "                skels_to_save)\n",
    "    finally:\n",
    "        video_writer.stop()\n",
    "        print(\"Program ends\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "suspended-collapse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
